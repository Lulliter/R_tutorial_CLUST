---
title: "NotesOnClusterAnaysis"
author: "Luisa M Mimmi"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---
 

# Intro
This is a recap of what I have learned on cluster analysis. I basically reproduced examples found in various sources (acknowledged at bottom), to learn how to implement the algorithms in R.

# Brief theory about Cluster Analysis

GOAL: putting _like with like_
It is considered `Exploratory Data Mining` bc it helps tease out relationships in large datasets 

DIFFERENCE FROM CLASSIFICATION: `Classification` identifies and matches obs. with EXISTING label/group (spam vs non spam / cats vs dogs) -- `Clustering` is "pragmatic grouping" done for a reason that make sense to me. (**THESE ARE NOT NATURAL GROUPINGS, THESE ARE GROUPS OF CONVENIENCE**). Its success depends on how well it serves my purpose. 

(*) RESULTS WILL CHANGE A LOT DEPENDING ON THE VARIABLES YOU USE!!!!!!
(*) Some of the algorithm REQUIRE CONTINUOUS DATA 


EXAMPLES:

+ marketing segmentation  - similar customers to recommend the same things
+ medicine - similar patients
+ music - genre on playlist that I want together

ALGORITHMs:

+ by **distance** (finds only convex clusters / these models are slow for big data...)
	- euclidean distance
	- connectivity models
	- hierarchical diagrams
	- joining or splitting

+ by **distance from centroid**  (only convex clusters / generally of similar size, you must pick k)
`centroid is defined by a mean`
	- (mean for every group)
	- k-means clustering

+ by **density of data** (connects dense regions and draw a perimeter around / )
	- connected dense regions in k-dimensions
 	+ it can model NON convex clusters
	+ it is possible to sort of ignore outliers 
	- hard to describe (by parametric distrib function)
	
+ by **distributional model**
	- clusters as statistical distributions 
	 (ex. multivariate normal)
	- prone to overfitting
	- good to see correlations b/w attributes in data 
	- limited to convex clusters 


## Three major kinds of clustering:
   1. `Hierarchical`: Start separate and combine
   2. Split into set number of clusters (e.g., `kmeans`)
   3. Dividing: Start with a single group and split


# EXAMPLES

I am using 2 differnt datasets:

	1. a dataset `states` with searches made in collected at US states and information about 5 personality traits. Data and hypothesis taken from "Divided We Stand: Three Psychological Regions of the United States andTheir Political, Economic, Social, and Health Correlates" https://www.apa.org/pubs/journals/releases/psp-a0034434.pdf
	2. a (subset of a) bank marketing dataset `bank`


## Set up 
```{r setup, eval= TRUE, echo = FALSE, message=FALSE, warning=FALSE}
## I am  executing bc it for the blog post
# knitr::opts_chunk$set(
# 	echo = TRUE,
# 	message = FALSE,
# 	warning = FALSE,
# 	results = "hide",
# 	tidy = TRUE,
# 	fig.show = "asis",
# 	fig.align = "center")

# packg
if (!require("pacman")) install.packages("pacman")
p_load(here,
		 cluster,dendextend,fpc,
		 stats,
		 Rtsne, 
		 dplyr,
		 ggplot2,
		 readr)

	# PerformanceAnalytics,
	# 	 ggcorrplot,
	# 	 ggpubr,
	# 	 GGally,
	# 	 kableExtra,
	# 	 pander, 
	# 	 gmodels, 
	# 	 caret, 
	# 	 formatR,
	# 	 e1071,
	# 	 caTools)

# LOAD dataset 1) ################################################
states <- read.csv(here::here( "states.csv"), header = T)


# Load dataset 2) 
df <- read_csv2(here::here("bank.csv") )
# I take a smaller sub-sample N =400
bank <- df[sample(nrow(df), 400), ]

rm(  df)
```

# HIERARCHICAL CLUSTERING 

### IV.a) Hierarchical clustering / `stats::hclust` / data = `states`

Source Linkedin Data Mining Course (DM_03_03.R)
STEPS:
	1. Create distance matrix (Euclidean)
	2. PErform hierarchical cluster analysis on a set of dissimilarities (distance)
	3. Plot 

(*) Here only numeric variables are considered

```{r}
# Check
colnames(states)

# Save numerical data only
st <- states[, 3:27]

# use the State Abbreviation as Index of rows (so they don't get analized)
row.names(st) <- states[, 2] # set row names for df 
colnames(st)

# CLUSTERING  (hierarchical clustering)###############################################

# 1) Create distance matrix (Euclidean)
d <- dist(st)

# 2) Hierarchical clustering
c <- hclust(d, method = "complete") # or one of "ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).

c # Info on clustering

# 3.a) Plot dendrogram of clusters
plot(c  , main = "Cluster with All Searches and Personality")

# 4.a) Cut the tree (resulting from hclust) into several groups  
    # Naming the file
    png(file = "graphs/dendrogramALLsearches.png")
# plot 
plot(c) # , main = "Cluster with All Searches and Personality")
 

# 4) Cut the tree (resulting from hclust) into several groups         
# Put observations in groups
# Need to specify either k = groups or h = height
g3 <- cutree(c, k = 3)  # "g3" = "groups 3"
# cutree(hcmt, h = 230) will give same result
g3

# Or do several levels of groups at once
# "gm" = "groups/multiple"
gm <- cutree(c, k = 2:5) # or k = c(2, 4)
gm

# Draw boxes around clusters
rect.hclust(c, k = 2, border = "gray")
rect.hclust(c, k = 3, border = "blue")
rect.hclust(c, k = 4, border = "green4")
rect.hclust(c, k = 5, border = "darkred")
    #Saving the file
   dev.off() 



# 3.b) Plot dendrogram of clusters
dend <- c %>% stats::as.dendrogram()
    # Naming the file
    png(file = "graphs/dendrogramALLsearches2.png")
plot(dend)


# 4.b) Cut the tree (resulting from hclust) into several groups         
plot(dend)
dendextend::rect.dendrogram(dend, 
                k = 3,
                border = 2:4, # colors
                #density = 2, 
                text = c("1", "b", "miao"), 
                text_cex = 3
)

# or 
# Vectorize(rect.dendrogram, "k")(dend, 4:5, border = 6)
    #Saving the file
   dev.off() 
```
 


For comparison purpose, we create a df with Sports search data only
```{r}
  
        # sports <- st[, 8:11]
        # head(sports) 
        # 
        # # For comparison, TRY AND LOOK USING ONLY THE SPORT SEARCHES
        # # 1//2/3) Or nest commands in one line (for sports data)
        # #Naming the file
        # png(file = "graphs/dendrogramSPORTsearches.png")
        # plot(hclust(dist(sports)),
        # 	  main = "Sports Searches")
        # dev.off() #Saving the file


```


# k-means CLUSTERING

### V) k-means clustering / `function` / data = 
 
K-means clustering is an _unsupervised_  machine learning algorithm for clustering `n` observations into `k` clusters where k is predefined or user-defined constant. 
It finds the minimum total **distance** of obs. form the `centroid` of the (best) cluster (for all dimensions)

It is a _partitioning_ algorithm. 

CONDITIONS: 

+ (only convex clusters similar size, you pick k)
+  _hyperparameter_ `k` = the number of clusters and has to be set beforehand. k should be picked as a # that is in the bal park of what you are willing to act on (eg. 5 if I want 5 segments of donors)
+ Where (through an iterative approach) it finds:
	+ The distance between data points within clusters should be as small as possible.
   + The distance of the centroids (= centres of the clusters) should be as big as possible.
   
APPLICATIONS: 
data has a smaller number of dimensions, is numeric, and is continuous. such as document clustering, identifying crime-prone areas, customer segmentation, insurance fraud detection, public transport data analysis, clustering of IT alerts…etc.

(*) We can scale/standardize the variables before clustering the data --> give same importance to each variable in caluculating distances


### V.a) k-means clustering / `stats::kmeans` / data = mtcars
```{r kmean}
bank1 <- bank[, c(1,6, 12, 13, 14)]  # Select num variables

# ====================================================== OPION k = 3 
km1 <- stats::kmeans(bank1, centers=3)
km1


# Graph based on k-means
cluster::clusplot(bank1,  # data frame
         km1$cluster,  # cluster data
         color = TRUE,  # color
#          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2)  # Labels clusters and cases

# here I get the centers
km1$centers

# here I get the cluster assignment (vector)
km1$cluster

# You can evaluate the clusters by looking at $totss and $betweenss.
km1$totss
km1$betweenss # sum of the squared distance between cluster centers.  Ideally you want cluster centers far apart from each other.
km1$withinss # sum of the square of the distance from each data point to the cluster center.  Lower is better. (high withinss would indicate either outliers are in your data or you need to create more clusters.)


# ====================================================== OPION k = 5 
# k-means clustering - I pick 5 clusters 
km2 <- kmeans(bank1, 5)
km2

# Graph based on k-means
clusplot(bank1,  # data frame
         km2$cluster,  # cluster data
         color = TRUE,  # color
#          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2)  # Labels clusters and cases

 # here I get the centers
km2$centers

# You can evaluate the clusters by looking at $totss and $betweenss.
km2$totss
km2$withinss # sum of the square of the distance from each data point to the cluster center.  Lower is better. (high withinss would indicate either outliers are in your data or you need to create more clusters.)
km2$betweenss # sum of the squared distance between cluster centers.  Ideally you want cluster centers far apart from each other.

 
# ======================================================  Finding optimal k -->  5 
# ----- Find ideal k
rng <- 2:20 # range of k =  from 2 to 20
tries <- 100 #Run the k Means algorithm 100 times
avg.totw.ss <-integer(length(rng)) #Set up an empty vector to hold all of points

for(v in rng){ # For each value of the range variable
 v.totw.ss <-integer(tries) #Set up an empty vector to hold the 100 tries
 for(i in 1:tries){
 k.temp <-kmeans(bank1, centers=v) #Run kmeans
 v.totw.ss[i] <-k.temp$tot.withinss#Store the total withinss
 }
 avg.totw.ss[v-1] <-mean(v.totw.ss) #Average the 100 total withinss
}

plot(rng,avg.totw.ss,type="b", main="Total Within SS by Various K",
 ylab="Average Total Within Sum of Squares",
 xlab="Value of K")

# Somewhere around K = 5 we start losing dramatic gains.  So I’m satisfied with 5 clusters.
```
(*) Looks like I might need to rescale to get meaningful clusters 



#### What if I rescale the data? 
```{r kmeanRESCALED}
# ========== standardize variables 
bank1_scaled<- scale(bank1) 

 
# ==========  Re-cluster on bank1_scaled df (k = 5)
# k-means clustering - I pick 5 clusters 
km3 <- kmeans(bank1_scaled, 5)
km3

# ========== Plot clusters  based on k-means -- 2 most important variables 
clusplot(bank1_scaled,  # data frame
         km2$cluster,  # cluster data
         color = TRUE,  # color
          shade = TRUE,  # Lines in clusters
         lines = 3,  # Lines connecting centroids
         labels = 2
         )  # Labels clusters and cases

# =========== Centroid Plot against 1st 2 discriminant functions
library(fpc)
fpc::plotcluster(x = bank1_scaled, km3$cluster) 
```


### V.b) k-means clustering / `function` / data = 
Source: http://blog.ephorie.de/learning-data-science-understanding-and-using-k-means-clustering

http://www.learnbymarketing.com/tutorials/k-means-clustering-in-r-example/

Because there are too many possible combinations of all possible clusters comprising all possible data points k-means follows an iterative approach called **_expectation-maximization algorithm_**.:
    1. Initialization: assign clusters randomly to all data points
    2. E-step (for expectation): assign each observation to the “nearest” (based on Euclidean distance) cluster
    3. M-step (for maximization): determine new centroids based on the mean of assigned objects
    4. Repeat steps 3 and 4 until no further changes occur

```{r kmmenSTEP}
n <- 3 # no. of centroids
set.seed(1415) # set seed for reproducibility
 
M1 <- matrix(round(runif(100, 1, 5), 1), ncol = 2)
M2 <- matrix(round(runif(100, 7, 12), 1), ncol = 2)
M3 <- matrix(round(runif(100, 20, 25), 1), ncol = 2)
M <- rbind(M1, M2, M3)
 
C <- M[1:n, ] # define centroids as first n objects
obs <- length(M) / 2
A <- sample(1:n, obs, replace = TRUE) # assign objects to centroids at random
colors <- seq(10, 200, 25) 

# helper function for plotting the steps 
clusterplot <- function(M, C, txt) {
  plot(M, main = txt, xlab = "", ylab = "")
  for(i in 1:n) {
    points(C[i, , drop = FALSE], pch = 23, lwd = 3, col = colors[i])
    points(M[A == i, , drop = FALSE], col = colors[i])    
  }
}
clusterplot(M, C, "Initialization")

#------------k-means algorithm iterative
# diamonds are the Centroids

repeat {
  # calculate Euclidean distance between objects and centroids
  D <- matrix(data = NA, nrow = n, ncol = obs)
  for(i in 1:n) {
    for(j in 1:obs) {
      D[i, j] <- sqrt((M[j, 1] - C[i, 1])^2 + (M[j, 2] - C[i, 2])^2)
    }
  }
  O <- A
   
  ## E-step: parameters are fixed, distributions are optimized
  A <- max.col(t(-D)) # assign objects to centroids
  if(all(O == A)) break # if no change stop
  clusterplot(M, C, "E-step")
   
  ## M-step: distributions are fixed, parameters are optimized
  # determine new centroids based on mean of assigned objects
  for(i in 1:n) {
    C[i, ] <- apply(M[A == i, , drop = FALSE], 2, mean)
  }
  clusterplot(M, C, "M-step")
}

#Check results 
(custom <- C[order(C[ , 1]), ])
##        [,1]   [,2]
## [1,]  3.008  2.740
## [2,]  9.518  9.326
## [3,] 22.754 22.396


#------------k-means algorithm BASE R
cl <- kmeans(M, n)
clusterplot(M, cl$centers, "Base R")

#Check results 
(base <- cl$centers[order(cl$centers[ , 1]), ])
##     [,1]   [,2]
## 2  3.008  2.740
## 1  9.518  9.326
## 3 22.754 22.396

# same!
```


### VI)  PAM (partitioning around medoids) clustering/ `function` / data = bank   
Source: https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3
copied from(?)http://dpmartin42.github.io/posts/r/cluster-mixed-types 

#### Example k-medoids (Partitioning around medoids) with bank marketing dataset
In the context of unsupervised classification, we may need to analyze datasets made of `mixed-type data`, where numeric, nominal (categorical, not ordered) or ordinal (categorical, ordered) features coexist. 

+ similarity measured with distance (`Gower distance` ~ avg of partial dissimilarity ranges )
+ PAM clustering algorithm (partitioning around medoids)
+ `k-medoid` is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori.
+ `silhouette coefficient` is used to determine optimal  # of clusters (we don't know a priori)
+ bank marketing dataset 

 

We first need to define some notion of (dis)similarity between observations. A popular choice for clustering is Euclidean distance, but only valid for continuous variables
We have to use a distance metric that can handle mixed data types --> `Gower distance`.
	
	* pros: Intuitive to understand and straightforward to calculate
	* cons: Sensitive to non-normality and outliers in continuous variables --> transformations as a pre-processing step might be necessary. 

Most similar and dissimilar clients according to Gower distance:
```{r}
# manipulation char -> factor
Factors <- c("job", "marital", "education", "default", "housing", "loan", "contact", "month", "poutcome", "y")
bank[ ,Factors] <-  lapply( bank[ ,Factors], as.factor)
glimpse(bank)

#' Compute Gower distance with the daisy {cluster} function
gower_dist <- daisy(bank, metric = "gower")
gower_mat <- as.matrix(gower_dist)

# Check...
#' Print most similar clients
bank[which(gower_mat == min(gower_mat[gower_mat != min(gower_mat)]), arr.ind = TRUE)[1, ], ]

#' Print most dissimilar clients
bank[which(gower_mat == max(gower_mat[gower_mat != max(gower_mat)]), arr.ind = TRUE)[1, ], ]
 
```

We use `Partitioning around medoids  (PAM) clustering algorithm` = an iterative clustering procedure with the following steps:

   1) Choose k random entities to become the medoids
   2) Assign every entity to its closest medoid (using our custom distance matrix in this case)
   3) For each cluster, identify the observation that would yield the lowest average distance if it were to be re-assigned as the medoid. If so, make this observation the new medoid.
   4)  If at least one medoid has changed, return to step 2. Otherwise, end the algorithm.

But first, we look for right number of clusters with `silhoutte width` [-1, 1] (where higher is better)
```{r}
sil_width <- c(NA)

# calculating silhouette width for clusters ranging from 2 to 8 for the PAM algorithm
for(i in 2:8){  
  pam_fit <- pam(gower_dist, diss = TRUE, k = i)  
  sil_width[i] <- pam_fit$silinfo$avg.width  
} 

# Plot sihouette width (higher is better)
plot(1:8, sil_width,
   xlab = "Number of clusters",
   ylab = "Silhouette Width")
lines(1:8, sil_width)

```
The `silhouette coefficient` contrasts the average distance to elements in the same cluster with the average distance to elements in other clusters. Objects with a high silhouette value are considered well clustered, objects with a low value may be outliers. This index works well with k-medoids clustering, and is also used to determine the optimal number of clusters. Please read the Wikipedia page for further details around computation and interpretation.

here --> pick 3 

Cluster Interpretation

```{r}
#Summary of each cluster
k <- 3
pam_fit <- pam(gower_dist, diss = TRUE, k)

pam_results <- bank %>%
  mutate(cluster = pam_fit$clustering) %>%
  group_by(cluster) %>%
  do(the_summary = summary(.))

pam_results$the_summary


```

Here one can attempt to derive some common patterns for clients within a cluster. As an example, cluster 1 is made of “management x tertiary x no default x no housing” clients, cluster 2 is made of “blue-collar x secondary x no default x housing” clients, etc.

```{r}
# BTW, in PAM alg, medoids serve as exemplars of each clusters
bank[pam_fit$medoids,]
```



Visualization in a lower dimensional space wwith t-distributed stochastic neighborhood embedding, or `t-SNE`
```{r}
# t-SNE = dimension reduction technique that tries to preserve local structure to make clusters visible in a 2D or 3D visualization. While typically utilizes Euclidean distance, it can handle a custom distance metric  
tsne_obj <- Rtsne(gower_dist, is_distance = TRUE)

tsne_data <- tsne_obj$Y %>%
  data.frame() %>%
  setNames(c("X", "Y")) %>%
  mutate(cluster = factor(pam_fit$clustering))

ggplot(aes(x = X, y = Y), data = tsne_data) +
  geom_point(aes(color = cluster))
```

Alternatives (for clustering mixed data):
https://www.researchgate.net/publication/323422280_kamila_Clustering_Mixed-Type_Data_in_R_and_Hadoop



***





# Resources 

+  LinkedIn Premium course "Data Mining"https://www.linkedin.com/learning/data-science-foundations-data-mining/clustering-in-r
+ https://www.kdnuggets.com/2016/09/comparing-clustering-techniques-concise-technical-overview.html
+ http://www.learnbymarketing.com/tutorials/ (kmeans clustering)
+ https://www.datasciencecentral.com/profiles/blogs/14-great-articles-and-tutorials-on-clustering
+ IADB ML course 
+ Hastie, Tibshirani, Firedman "The elements of Statistical Learning", 2009

